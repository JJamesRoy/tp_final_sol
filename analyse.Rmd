```{r}
library(tidyverse)
library(tree)
```

```{r, warning=FALSE}
dat = read.csv("data.csv")

dat[1:147] = sapply(dat[1:147], as.numeric)
# Convertir en numeric à la place de character

dat_full = dat %>% mutate_at(vars(-c(11:13)), ~round(ifelse(is.na(.), mean(., na.rm = TRUE), .), 0))
# Imputer les données manquantes. Les colonnes exclues le sont car elles sont largement composé de NA et ça serait une mauvaise idée de les imputer

#dat_full[,-c(1,2,3,10,11,12,13)] = sapply(dat_full[,-c(1,2,3,10,11,12,13)], as.factor)

#dat_full[sapply(dat_full, is.character)] <- lapply(dat_full[sapply(dat_full, is.character)], as.factor)
# Changer la classe des variables catégorielles en facteur (ça bug donc en 2 fonctions)
```

```{r}
dat_full_clean = dat_full %>% select(c(age, niveau, sexe, statmar, travp, scolp, travm, scolm, negp1, negp8, negp9, negm1, negm8, negm9, dets1, mus2, mushr, alien1, alien2, sui1, drog1, drog2, drog3, drog4))
# Sélectionner les variables théoriquement intéressante
```

```{r}
corr_matrix <- cor(dat_full_clean)

# Sélectionner les paires de variables avec une corrélation supérieure à 0,9
high_corr_pairs <- which(corr_matrix > 0.9 & corr_matrix < 1, arr.ind = TRUE)

# Extraire les noms de variables correspondant aux paires de variables sélectionnées
high_corr_varnames <- unique(colnames(dat_full_clean)[high_corr_pairs[,2]])

# Afficher le vecteur de noms de variables avec une corrélation supérieure à 0,9
high_corr_varnames
```

```{r}
dat_lm = dat_full_clean %>% 
  mutate(sexe = ifelse(sexe == 2,1, 0), # 1 si gars, 0 sinon
         sui1 = ifelse(sui1 == 1, 1, 0), # 1 si envie de suicide, 0 sinon
#         statmar = ifelse(statmar == 3, 0,1), #0 si parent divorcé
#         travp = ifelse(travp == 1, 1, 0), # 1 si pere travail temps plein, 0 sinon
#         scolp = ifelse(scolp == 4 | scolp == 5 | scolp == 6, 1, 0), # 1 si pere plus que sec 5, 0 sinon
#         travm = ifelse(travm == 1, 1, 0), # 1 si mere travail temps plein, 0 sinon
#         scolm = ifelse(scolm == 4 | scolm == 5 | scolm == 6, 1, 0), # 1 si mere plus que sec 5, 0 sinon
#         negp1 = ifelse(negp1 >= 2, 1, 0), # 1 si pere interessé, 0 sinon
#         negp8 = ifelse(negp8 >= 2, 1, 0), # 1 si pere aidé, 0 sinon
#         negp9 = ifelse(negp9 >= 2, 1, 0), # 1 si pere prend le temps de discuté, 0 sinon
#         negm1 = ifelse(negm1 >= 2, 1, 0), # 1 si mere interessé, 0 sinon,
#         negm8 = ifelse(negm8 >= 2, 1, 0), # 1 si mere aidé, 0 sinon,
#         negm9 = ifelse(negm9 >= 2, 1, 0), # 1 si mere prend le temps de discuté, 0 sinon, 
#         dets1 = ifelse(dets1 >= 2, 1, 0), # 1 si senti despéré, 0 sinon
#         metal = ifelse(mus2 >= 4, 1, 0), # 1 si aime le metal, 0 sinon
#         alien1 = ifelse(alien1 >=4, 1, 0), # 1 si pense que pas de but à être en vie, 0 sinon
#         alien2 = ifelse(alien2 >=4, 1, 0), # 1 si pense que bon à rien, 0 sinon
         drog = round((drog1 + drog2 + drog3 +drog4)/4)) # 1 si consomme de la drogue (pour alcool, +1 car normal de boire une fois de l'alcool), 0 sinon

dat_lm = dat_lm %>% select(-c("drog1", "drog2", "drog3", "drog4"))
dat_lm_num = dat_lm

sui1 = factor(dat_lm$sui1)
dat_lm[,-c(1,17)] = sapply(dat_lm[,-c(1,17)], as.factor)
dat_lm[sapply(dat_lm, is.character)] <- lapply(dat_lm[sapply(dat_lm, is.character)], as.factor)
```

```{r}
tree.model <- tree(sui1 ~ ., dat_lm)
summary(tree.model)
```

```{r}
plot(tree.model)
text(tree.model, pretty = 0)
```

```{r}
set.seed(1)
train <- sample(1:nrow(dat_lm), 210)
test <- dat_lm[-train, ]
test_num <- dat_lm[-train, ]
sui1.test <- sui1[-train]
tree.model <- tree(sui1 ~ ., dat_lm,
    subset = train)
tree.pred <- predict(tree.model, test,
    type = "class")
table(tree.pred, sui1.test)
(32 + 23) / 94
```

```{r}
cv.model <- cv.tree(tree.model, FUN = prune.misclass)
names(cv.model)
cv.model
```

```{r}
par(mfrow = c(1, 2))
plot(cv.model$size, cv.model$dev, type = "b")
plot(cv.model$k, cv.model$dev, type = "b")
```

```{r}
prune.model <- prune.misclass(tree.model, best = 6)
plot(prune.model)
text(prune.model, pretty = 0)
```

```{r}
tree.pred <- predict(prune.model, test,
    type = "class")
table(tree.pred, sui1.test)
(34 + 26) / 94
```

```{r}
prune.model <- prune.misclass(tree.model, best = 8)
plot(prune.model)
text(prune.model, pretty = 0)
tree.pred <- predict(prune.model, test,
    type = "class")
table(tree.pred, sui1.test)
(36 + 23) / 94
```



## BAGGING

```{r}
library(randomForest)
bag.model <- randomForest(sui1 ~ ., data = dat_lm,
    subset = train, mtry = 20, importance = TRUE)
bag.model
plot(bag.model)
```


```{r}
bag.pred <- predict(bag.model, test,
    type = "class")
table(bag.pred, sui1.test)
(36 + 29) / 94
```

```{r}
bag.importance = as.data.frame(importance(bag.model))
varImpPlot(bag.model, type = 2)
```

```{r}
p = round(sqrt(20))
rf.model <- randomForest(sui1 ~ ., data = dat_lm,
    subset = train, mtry = 4, importance = TRUE)
rf.model
plot(rf.model)
```

```{r}
rf.pred <- predict(rf.model, test,
    type = "class")
table(rf.pred, sui1.test)
(40 + 26) / 94
```


```{r}
rf.importance = as.data.frame(importance(rf.model))
varImpPlot(rf.model, type = 2)
```

## BOOSTING

```{r}
library(gbm)
set.seed(1)
boost.model <- gbm(sui1 ~ ., data = dat_lm_num[train, ],
    distribution = "bernoulli", n.trees = 5000,
    interaction.depth = 4)

summary(boost.model)
```


```{r}
boost.pred <- predict(boost.model, newdata = dat_lm_num[-train,],
    type = "response")
table(round(boost.pred), sui1.test)
(32 + 26) / 94
```


